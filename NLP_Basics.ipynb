{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9d3176a-835d-4d96-9e0e-5421285b4d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltkNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "     -------------------------------------- 302.2/302.2 kB 2.7 MB/s eta 0:00:00\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Collecting click\n",
      "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "     -------------------------------------- 97.9/97.9 kB 622.0 kB/s eta 0:00:00\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2023.12.25-cp310-cp310-win_amd64.whl (269 kB)\n",
      "     ------------------------------------ 269.5/269.5 kB 182.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\shyam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.7 joblib-1.3.2 nltk-3.8.1 regex-2023.12.25 tqdm-4.66.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a03a0264-1800-4253-9495-299f4223f9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04275cf0-4452-472f-a47f-4dfea192f97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\SHYAM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\SHYAM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\SHYAM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\SHYAM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\SHYAM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\SHYAM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\SHYAM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\SHYAM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\SHYAM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\SHYAM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\SHYAM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\SHYAM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\SHYAM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\SHYAM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\SHYAM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\SHYAM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\SHYAM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\SHYAM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\SHYAM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\SHYAM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\SHYAM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\SHYAM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507b850f-6449-4741-a2d8-8425e29f4523",
   "metadata": {},
   "source": [
    "### 1. Tokenization\n",
    "#### Process of breaking down a text paragraph into smaller chunks of sentences and words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4506dc07-5651-429d-ba69-792efabe12ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8e6d63b-8969-40d2-8eb8-0e6df9d4f0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "paragraph = \"\"\"Hello Participants ! Topic of the day is Natural Language Processing. We are learning about Natural Language Processing using nltk package. natural language processing is interesting and easy. This is also known as NLP.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f14dea25-76d9-4495-83a0-c6ec28f1877e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Participants !',\n",
       " 'Topic of the day is Natural Language Processing.',\n",
       " 'We are learning about Natural Language Processing using nltk package.',\n",
       " 'natural language processing is interesting and easy.',\n",
       " 'This is also known as NLP.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text = sent_tokenize(paragraph)\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca58148c-5234-4bef-9fdb-dd20504f64e5",
   "metadata": {},
   "source": [
    "### 2. Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ed7c6d8-9b50-469d-b529-676143154ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9004b5b5-8652-4b4b-af7d-274fbf141466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Participants', '!', 'Topic', 'of', 'the', 'day', 'is', 'Natural', 'Language', 'Processing', '.', 'We', 'are', 'learning', 'about', 'Natural', 'Language', 'Processing', 'using', 'nltk', 'package', '.', 'natural', 'language', 'processing', 'is', 'interesting', 'and', 'easy', '.', 'This', 'is', 'also', 'known', 'as', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(paragraph))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc1f200-5d94-4b56-b21d-5de0f51bc54c",
   "metadata": {},
   "source": [
    "### 3. Tokens and Removing Punctuations\n",
    "#### Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66a05d04-7a22-4b84-8f9b-2b1441f1298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9b91034-f05b-4f52-b071-543e84415297",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc464d1b-892f-4307-a277-3807592202f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Participants',\n",
       " 'Topic',\n",
       " 'of',\n",
       " 'the',\n",
       " 'day',\n",
       " 'is',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'We',\n",
       " 'are',\n",
       " 'learning',\n",
       " 'about',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'using',\n",
       " 'nltk',\n",
       " 'package',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'is',\n",
       " 'interesting',\n",
       " 'and',\n",
       " 'easy',\n",
       " 'This',\n",
       " 'is',\n",
       " 'also',\n",
       " 'known',\n",
       " 'as',\n",
       " 'NLP']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokeizer.tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7af82a-0f45-47bf-8991-0a93093e22ad",
   "metadata": {},
   "source": [
    "### 4. Lowercase paragraph / Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f4562c2-607a-40f2-9267-ce4ec2f36156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello participants ! topic of the day is natural language processing. we are learning about natural language processing using nltk package. natural language processing is interesting and easy. this is also known as nlp.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph = paragraph.lower()\n",
    "paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c04fc04-8584-4550-9c32-613214e17027",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+') \n",
    "new_words = tokenizer.tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405d195d-846d-4870-8be5-5f9a78f85c47",
   "metadata": {},
   "source": [
    "### 5. Removing StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06b28ece-16b9-4131-9294-1734b28d5554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "# english, german, spanish, portuguese, dutch, italian, greek \n",
    "stop_words = stopwords.words(\"english\") \n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2bdb5c6-4eeb-45b4-bb36-a85420741b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'participants',\n",
       " 'topic',\n",
       " 'day',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'learning',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'using',\n",
       " 'nltk',\n",
       " 'package',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'interesting',\n",
       " 'easy',\n",
       " 'also',\n",
       " 'known',\n",
       " 'nlp']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_words = [] \n",
    "for w in new_words: \n",
    "    if w not in stop_words: \n",
    "        filtered_words.append(w)\n",
    "\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781512c8-eb4a-4eed-a4af-5aaa35050cc5",
   "metadata": {},
   "source": [
    "### 6. Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c74c9d3-4bf6-414c-968a-12093afa79f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (300106211.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[19], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    from nltk.probability\u001b[0m\n\u001b[1;37m                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability \n",
    "import FreqDist \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b3afd9-e736-45ee-8f33-c8eb6341f2eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
